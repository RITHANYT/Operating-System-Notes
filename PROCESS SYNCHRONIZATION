*************************Process Synchronization in Operating System************************************

Process Synchronization is a mechanism used in Operating Systems to ensure that multiple processes
or threads can safely access shared resources or data, without conflict.
There are two ways any process can execute ‚Äì
1.In Concurrent Execution ‚Äì the CPU scheduler switches rapidly between processes. 
A process is stopped at any points and the processor is assigned to another instruction execution. Here, only one 
instruction is executed at a time.
2.Parallel execution ‚Äì 2 or more instructions of different process execute simultaneously on different processing cores.

When processes run concurrently (especially on multi-core systems), they may:
---Access shared memory (e.g., account balance, files, tables).
---Cause data inconsistency if not synchronized.

********üìå Without synchronization: One process may override another‚Äôs update ‚ûù 
leads to wrong or unpredictable results (called race condition).
EX: Imagine you have $5000 in your bank account.
Net Banking starts at t = 0ms
‚ûù sees $5000, proceeds to withdraw $4000
ATM starts at t = 0.5ms
‚ûù still sees $5000 (since Net Banking hasn‚Äôt completed), proceeds to withdraw $4000
ATM withdraws $4000 at t = 1.5ms
Net Banking finishes at t = 2ms
üí• Total Withdrawn: $8000 ‚ûù Not possible!
‚úÖ This happened because both processes accessed the balance at the same time ‚Äî no synchronization!

Why Synchronization is needed:
1.To avoid race conditions
2.To prevent inconsistent data
3.To maintain system stability and integrity

Process Types:
Independent	:        Process does not affect or get affected by other processes
Cooperative	:        Process depends on others or shares data/resources

Every process has 4 sections in synchronization problems:
1.Entry Section ‚Äì Code to request access to critical section
2.Critical Section ‚Äì Shared data is accessed or modified here
3.Exit Section ‚Äì Code that signals that process is done
4.Remainder Section ‚Äì The rest of the code (non-critical)


 Solutions:
1.Mutual Exclusion ‚Äì Only one process in the critical section at a time
2.Progress ‚Äì If no one is in the critical section, the next process should enter
3.Bounded Waiting ‚Äì A process should not wait forever to enter the critical section

**********************************************************Inter-Process Communication (IPC)***************
Inter-Process Communication (IPC) is a mechanism that allows processes to communicate and share data with each other.
It‚Äôs mainly used when cooperating processes need to exchange information.

There are 2 main methods used for IPC:
1. üß† Shared Memory
-----A portion of memory is shared between processes.
-----Processes can read/write data directly in this shared space.
-----Faster, because after setup, no kernel involvement is needed.
----Requires synchronization (e.g., semaphores or mutexes) to avoid race conditions.

2. üíå Message Passing
Processes send and receive messages through system calls.
Messages may be sent synchronously or asynchronously.
Easier to implement, and safer than shared memory (no direct access).

****************************************************************Critical Section********************************************************
A Critical Section is the portion of code where shared resources (like variables, files, memory)
are accessed.
‚ö†Ô∏è Problem: If multiple processes enter the critical section at the same time, it can lead to inconsistent 
data, race conditions, or even deadlocks.
To safely manage the critical section and avoid issues like deadlock or starvation, a solution must satisfy the following 3 conditions:

Condition	Description
1. Mutual Exclusion	: Only one process is allowed inside the critical section at any time.
2. Progress	: If no process is inside the CS, then processes must not be blocked unnecessarily and should be allowed to enter.
3. Bounded Waiting :	A process requesting to enter the CS will eventually get the chance, i.e., no starvation.

****************************************KERNEL HANDLING IN OS**************************************
What is Kernel Handling in the OS?
The kernel is the core part of the operating system. It manages things like:
1.CPU time
2.Memory
3.Input/Output devices
4.Running programs (called processes)

When a program (process) runs and needs to do something important like accessing shared data, it may need help from the kernel.
Now, how the kernel handles multiple processes trying to run at the same time ‚Äî especially in the Critical Section ‚Äî is important.

‚úåÔ∏è Two Ways the Kernel Can Handle Processes
1. üîÑ Preemptive Kernel
Think of it like a classroom debate where anyone can be interrupted anytime.
A process running in the kernel mode can be paused (interrupted) at any time.
This is great for real-time systems or responsive systems like games, or banking apps.
But there's a risk! If two processes are accessing shared data, they might interfere with each other!

üí• Race Around Condition Example
Let‚Äôs say two people are trying to write on the same whiteboard at the same time:

Person A writes ‚ÄúHello‚Äù

Person B writes ‚ÄúWorld‚Äù right over it

Now the board shows something weird like ‚ÄúHeWorld‚Äù ü§Ø ‚Äî that‚Äôs a Race Condition.

2. ‚è∏Ô∏è Non-Preemptive Kernel
Think of it like a person giving a speech ‚Äî others wait till they finish before speaking.

Once a process starts running in kernel mode, no one else can interrupt.

This prevents the race condition, because only one person (process) is modifying data at a time.

It‚Äôs safer but slower, because other processes must wait their turn.

*****************************************MUTEX********************************************

Mutex is binary ‚Üí It‚Äôs either locked (1) or unlocked (0)
When a thread wants to use a shared resource:
It locks the mutex.
Uses the resource.
Releases the mutex when done.

Feature	                                         Description
Binary	                                     Can be 0 or 1 (locked or unlocked)
Lock/Unlock	                                  Supports lock() and unlock()
Threads	                                    Designed mainly for threads within a process
User-space	                                  Works in user-space programs
Ownership	                                   Only the thread that locks can unlock
One at a time	                             Only one thread can hold the lock

*****************************************Semaphore************************************************
This is a tool to manage access to shared resources by multiple processes or threads,
especially to avoid race conditions in the critical section.

There are two types:
1.Binary Semaphore (0 or 1): Acts like a mutex
2.Counting Semaphore (0 to N): Allows multiple accesses (like multiple permits)

There are two key operations:
1. wait() or P(s)
Also called the decrement operation.

2.2. signal() or V(s)
Also called the increment operation.

//////////example:
// Counting semaphore initialized with value = 3

wait(S);     // S becomes 2 (1st person enters)
wait(S);     // S becomes 1 (2nd person enters)
wait(S);     // S becomes 0 (3rd person enters)
wait(S);     // BLOCKED (4th person must wait until someone exits)

signal(S);   // S becomes 1 (a person leaves, 4th can now enter)


//////Example: Binary Semaphore
Let‚Äôs say only one process should access the critical section at a time.
Semaphore S = 1;   // Binary Semaphore initialized to 1

Process A:
wait(S);
// Critical Section
signal(S);

Process B:
wait(S);
// Critical Section
signal(S);
üõë Important Notes
Semaphores do not prevent deadlock‚Äîthey help manage access but must be used carefully.
The busy-wait loop (while (S <= 0);) can be replaced with blocking mechanisms in real 
systems to save CPU.

*********************************************Spinlock************************************
A Spinlock is a lock where the thread actively waits in a loop ("spins") until the 
lock becomes available.
Instead of sleeping (like in a semaphore), the thread continuously checks the lock 
variable in a loop.



Feature	                      Mutex	                 Semaphore	                          Spinlock
Used for	                   Threads	                 Processes	                          Threads
Type	                        Binary	             Binary / Counting	                         Binary
Operations	             lock(), unlock()	       wait(), signal()	                    lock(), unlock()
User/Kernel Space	       User Space	           Kernel Space	                         Mostly Kernel/User
Ownership	          Only one thread at a time	  Multiple processes (counting)	        Only one thread at a time
CPU Utilization	     Low (if thread sleeps)	      Low (can block process)	            High (busy waiting)
Best for	             Mutual Exclusion (Thread)	    Synchronization (Processes)	    Short Locks in SMP systems

********************************************Atomic operation************************************
An Atomic Operation is an operation that either happens completely or not 
at all ‚Äî and nothing can interfere while it's happening.
They help us avoid race conditions in multithreaded or multiprocess environments.
üèÅ What is a race condition?
A race condition occurs when:
Two or more threads/processes access and modify shared data at the same time,
The final result depends on the timing of who finishes first,
And that leads to incorrect or unpredictable results.
üìö Where Are Atomic Operations Used?
    1.Multithreading to update shared variables safely
    2.In OS kernels and device drivers
    3.In lock-free data structures like atomic queues
    4.In real-time systems where blocking (like locks) is bad
üß± Types of Atomic Operations:
1.Read/Write atomicity ‚Äì read or write to a memory location in one go.
2.Atomic Increment/Decrement
3.Compare-And-Swap (CAS) ‚Äì very important!

‚ú® Example of CAS (Compare-And-Swap):

int atomicCompareAndSwap(int* addr, int expected, int newValue);


If the value at addr equals expected, then it writes newValue.
Else, it does nothing.
This is used in lock-free algorithms to coordinate safely.


